{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "8YRSZ6kK3Qer",
    "outputId": "cfe3b3f9-fbf9-42bb-b773-af55b532fe20"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "from ast import literal_eval \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoHi4QdC5apa"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import regexp_replace,col,array_contains,explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import concat_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train data to pandas dataframe and dumping it to spark dataframe\n",
    "train_df = pd.read_csv(r'train.csv')\n",
    "train_spark_df = spark.createDataFrame(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4wMajSknMC3W"
   },
   "outputs": [],
   "source": [
    "#loading label data file that is generated initially to pandas dataframe and dumping it to spark dataframe\n",
    "lables_df = pd.read_csv(r'genres.csv')\n",
    "lables_spark_df = spark.createDataFrame(lables_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading test data to pandas dataframe and dumping it to spark dataframe\n",
    "test_pd_df = pd.read_csv(r'test.csv')\n",
    "test_spark_df = spark.createDataFrame(test_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZUNn84K21fS"
   },
   "outputs": [],
   "source": [
    "#using joins to combine both data frame with using row_id as key and dumped it into new dataframe\n",
    "x1 = train_spark_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "x2 = lables_spark_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "train_df = x1.join(x2, \"row_id\").drop(\"row_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using joins adding label columns to test data set\n",
    "x3 = test_spark_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "test_df = x3.join(x2, \"row_id\").drop(\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Z593o_L88AEK",
    "outputId": "6a3870e3-a024-4aef-cee9-190937b2be33",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 184 ms, sys: 58.1 ms, total: 242 ms\n",
      "Wall time: 8min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#adding regex tokenizer for splitting into words\n",
    "regex=RegexTokenizer(inputCol=\"plot\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "#adding own defined set of words\n",
    "add_stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "#removing stopwords\n",
    "stopwordsRemover=StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "#using word2vec as feature engineering\n",
    "word2Vec=Word2Vec(vectorSize=350,minCount=5,inputCol=\"filtered\",outputCol=\"features\")\n",
    "#adding all these to pipeline\n",
    "pipeline=Pipeline(stages=[regex,stopwordsRemover,word2Vec])\n",
    "#applying this pipeline on train dataframe\n",
    "op1=pipeline.fit(train_df)\n",
    "train_dataset=op1.transform(train_df)\n",
    "#applying this pipeline on test dataframe\n",
    "op2=pipeline.fit(test_df)\n",
    "test_dataset=op2.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bI_skgB9HaAT",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "CPU times: user 2min 15s, sys: 48.8 s, total: 3min 4s\n",
      "Wall time: 3h 36min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "finallist = []\n",
    "i=1\n",
    "labels = lables_spark_df.columns\n",
    "lr = LogisticRegression(featuresCol = 'features',maxIter=700)\n",
    "#iterating through lr model for 20 labels and storing predictions in list\n",
    "for col in labels:\n",
    "    lr.setLabelCol(col)\n",
    "    lrModel = lr.fit(train_dataset)\n",
    "    predictions = lrModel.transform(test_dataset)\n",
    "    predictions = predictions.withColumn(\"prediction\",F.col(\"prediction\").cast(IntegerType()))\n",
    "    finallist.append(predictions.select(\"movie_id\",\"prediction\"))\n",
    "    print(i)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6m2q85P2sqBl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 619 ms, sys: 179 ms, total: 799 ms\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#after making predictions which are stored in finallist now these predictions are appeneded to movie id\n",
    "df = [j.selectExpr('movie_id', f'prediction as prediction_{i}') for i,j in enumerate(finallist)]\n",
    "temp = reduce(lambda x, y: x.join(y, ['movie_id'], how='full'), df)\n",
    "col = ['prediction_%d' % i for i in range(len(finallist))]\n",
    "#appended movie id and predictions are now dumped to .csv file\n",
    "temp = temp.withColumn('predictions',concat_ws(\" \",*col)).drop(*col).toPandas().to_csv(\"part3.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DIC_PySpark_Part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
