{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "8YRSZ6kK3Qer",
    "outputId": "cfe3b3f9-fbf9-42bb-b773-af55b532fe20"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "from ast import literal_eval \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoHi4QdC5apa"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import regexp_replace,col,array_contains,explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import concat_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train data to pandas dataframe and dumping it to spark dataframe\n",
    "train_df = pd.read_csv(r'train.csv')\n",
    "train_spark_df = spark.createDataFrame(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading label data file that is generated initially to pandas dataframe and dumping it to spark dataframe\n",
    "lables_df = pd.read_csv(r'genres.csv')\n",
    "lables_spark_df = spark.createDataFrame(lables_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4wMajSknMC3W"
   },
   "outputs": [],
   "source": [
    "#loading test data to pandas dataframe and dumping it to spark dataframe\n",
    "test_pd_df = pd.read_csv(r'test.csv')\n",
    "test_spark_df = spark.createDataFrame(test_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZUNn84K21fS"
   },
   "outputs": [],
   "source": [
    "#using joins to combine both data frame with using row_id as key and dumped it into new dataframe\n",
    "x1 = train_spark_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "x2 = lables_spark_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "train_df = x1.join(x2, \"row_id\").drop(\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using joins adding label columns to test data set\n",
    "x3 = test_spark_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "test_df = x3.join(x2, \"row_id\").drop(\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+---------+--------+--------+--------------+----------+--------+--------------+---------------+--------+-----------------+-------+------------------+-----------+-------------+------------+----------------+-----------+---------+-----------------+---------+----------------+\n",
      "|movie_id|movie_name|                plot|    genre|# Drama | Comedy | Romance Film | Thriller | Action | World cinema | Crime Fiction | Horror | Black-and-white | Indie | Action/Adventure | Adventure | Family Film | Short Film | Romantic drama | Animation | Musical | Science Fiction | Mystery | Romantic comedy|\n",
      "+--------+----------+--------------------+---------+--------+--------+--------------+----------+--------+--------------+---------------+--------+-----------------+-------+------------------+-----------+-------------+------------+----------------+-----------+---------+-----------------+---------+----------------+\n",
      "|18549958|Love, Mary|Mary was a lonely...|['Drama']|       1|       0|             0|         0|       0|             0|              0|       0|                0|      0|                 0|          0|            0|           0|               0|          0|        0|                0|        0|               0|\n",
      "+--------+----------+--------------------+---------+--------+--------+--------------+----------+--------+--------------+---------------+--------+-----------------+-------+------------------+-----------+-------------+------------+----------------+-----------+---------+-----------------+---------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover , CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Z593o_L88AEK",
    "outputId": "6a3870e3-a024-4aef-cee9-190937b2be33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 151 ms, sys: 46.2 ms, total: 197 ms\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#adding regex tokenizer for splitting into words\n",
    "regex=RegexTokenizer(inputCol=\"plot\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "#adding own defined set of words\n",
    "add_stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "#removing stopwords\n",
    "stopwordsRemover=StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "#adding hashingTF to extract term doc values\n",
    "hashingTF=HashingTF(inputCol=\"filtered\",outputCol=\"rawfeatures\")\n",
    "#adding idf to extract idf values\n",
    "idf=IDF(inputCol='rawfeatures',outputCol='features',minDocFreq=5)\n",
    "#adding all these to pipeline\n",
    "pipeline=Pipeline(stages=[regex, stopwordsRemover, hashingTF,idf])\n",
    "#applying this pipeline on train dataframe\n",
    "op1=pipeline.fit(train_df)\n",
    "train_dataset=op1.transform(train_df)\n",
    "#applying this pipeline on test dataframe\n",
    "op2=pipeline.fit(test_df)\n",
    "test_dataset=op2.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0ZqhfSMApxm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(262144,[14699,16...|\n",
      "|(262144,[9129,120...|\n",
      "|(262144,[3834,420...|\n",
      "|(262144,[1133,183...|\n",
      "|(262144,[571,2089...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset.select('features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bI_skgB9HaAT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 s, sys: 4.76 s, total: 19.3 s\n",
      "Wall time: 1h 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "finallist=[]\n",
    "labels=lables_spark_df.columns\n",
    "lr=LogisticRegression(featuresCol='features',maxIter=700)\n",
    "#iterating through lr model for 20 labels and storing predictions in list\n",
    "for label in labels:\n",
    "    lr.setLabelCol(label)\n",
    "    lrModel=lr.fit(train_dataset)\n",
    "    predictions=lrModel.transform(test_dataset)\n",
    "    predictions=predictions.withColumn(\"prediction\",F.col(\"prediction\").cast(IntegerType()))\n",
    "    finallist.append(predictions.select(\"movie_id\",\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6m2q85P2sqBl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 632 ms, sys: 226 ms, total: 859 ms\n",
      "Wall time: 3min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#after making predictions which are stored in finallist now these predictions are appeneded to movie id\n",
    "df=[j.selectExpr('movie_id', f'prediction as prediction_{i}') for i,j in enumerate(finallist)]\n",
    "temp = reduce(lambda x, y: x.join(y, ['movie_id'], how='full'), df)\n",
    "col = ['prediction_%d' % i for i in range(len(finallist))]\n",
    "#appended movie id and predictions are now dumped to .csv file\n",
    "temp= temp.withColumn('predictions',concat_ws(\" \",*col)).drop(*col).toPandas().to_csv(\"part2.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DIC_PySpark_Part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
